---
title: "[Figure 5] Prepare variables for analysis of transcriptional effects of integration status"
output: html_notebook
---

This code prepares variables for differential expression analysis (deseq.Rmd), and to explore the data via Principal Components Analysis clustering.

# Set up Workspace

To ensure that this code is usable across a variety of platforms, the user-provided variables (e.g. the date of the run, the path to data files) are stored in the .csv file "userVars", which should be in the directory containing the directory that holds the .Rmd scripts.

## Import the variables you need
```{r}
# import .csv file with user paths and info written out
userVars <- read.delim("../userVars.csv", stringsAsFactors=FALSE, sep=",")

# print(userVars) # uncomment to include output in notebook to document variables in .html notebook output for this run

# user-determined variables used for generating filenames
currDate <- userVars[userVars$variable=='currentDate', 'path']
projDir <- userVars[userVars$variable=='projectDirectory', 'path']
pathToHTSeq <- userVars[userVars$variable=='pathToHTSeq', 'path']
pathToCleanClones <- userVars[userVars$variable=='pathToCleanClones', 'path']
pathToMetadata <- userVars[userVars$variable=='pathToMetadata', 'path']
pathToPCRIntegrationData<- userVars[userVars$variable=='pathToPCRIntegrationData', 'path']
pathTo16pGeneList <- userVars[userVars$variable=='pathTo16pGeneList', 'path']

```


## Make folders for this
```{r}
# name for files etc
expName <- 'setup'

# make folder to contain experiment output

## create folder for output
pathToAllProjectDir <- paste0(projDir, 'output/figure5/', expName)
dir.create(pathToAllProjectDir, recursive=TRUE)

## create folder for today's run
pathToProjectDir <- paste0(projDir, 'output/figure5/', expName,'/', currDate)
dir.create(pathToProjectDir)

## make folder to contain non-RData output
pathToProjectOutput <- paste0(pathToProjectDir,'/other')
dir.create(pathToProjectOutput)

## make folder for RData vars
pathToRData <- paste0(pathToProjectDir,'/RData')
dir.create(pathToRData)

```




# Import useful things


## Import Metadata
```{r}
# import metadata
library(readr)
metadata <- read_csv(pathToMetadata, na = "NA")
metadata <- with(metadata, metadata[order(factor(metadata$Genotype, levels=c('UNK','DEL','DUP', 'WT') ) ) ,])


# Make sampleTable to instruct DESeqDataSetFromHTSeqCount how to import data
## sampleTable = rows are samples; 
## columns are (first) count file generated by htseq, (second) file nicknames, (the rest) metadata
sampleTable_withUND <- data.frame( metadata[,c('DESeqAnalysisID', 'FileName',
                                               colnames(metadata)[!colnames(metadata) 
                                                                  %in% c('ASD','Origin')] ) ] )

# trim out unwanted samples from sampleTable

## remove anything with no file name in case of Excel spreadsheet error
sampleTable_withUND <- sampleTable_withUND[!(is.na(sampleTable_withUND$DESeqAnalysisID)),]

## remove UND and DUP samples for this run
sampleTable_DELvWT <- sampleTable_withUND[!(sampleTable_withUND$Genotype == 'UND' | sampleTable_withUND$Genotype == 'DUP'),] 

## only extract clean (non-integrated) clones
cleanClones <- read_csv(pathToCleanClones, na = 'NA', col_names=FALSE)
sampleTable <- subset(sampleTable_DELvWT, sampleTable_DELvWT$Line %in% unlist(cleanClones[,1]) )


# add factor for high/low IQ
sampleTable$IQbin <- 'Unknown'
sampleTable[sampleTable$IQ > 75 & !is.na(sampleTable$IQ),'IQbin'] <- 'Above75'
sampleTable[sampleTable$IQ < 75 & !is.na(sampleTable$IQ),'IQbin'] <- 'Below75'

# declare all rows except for first two as factors
sampleTable[,-c(1,2)] <- lapply( sampleTable[,-c(1,2)], factor )
# str(sampleTable) # uncomment to verify that the previous line worked



```

## Import 16p gene list
```{r}
# Import 16p Gene List
load(pathTo16pGeneList) # file under kristinmuench/geneList_16p112.RData

```

# Create dataset

## Import of Raw Count Data
```{r}
# install needed library
library(DESeq2)

# import all clean DEL and WT data into count table
myData <- DESeqDataSetFromHTSeqCount(sampleTable = sampleTable,
                                     directory = pathToHTSeq,
                                     design = ~ Sex + Genotype + GrowBatch ) # This design will change later in code

```

## Pre-cleaning Data
```{r}

# direct written outputs to designated project output folder
setwd(pathToProjectOutput)

# Collapse replicates, P20+P20 and P21+P21 - can treat as technical replicates because cluster similarly

# Note that WT_L1_P26 resembles itself less well.
myData <- collapseReplicates(myData, myData$Line, renameCols = TRUE)
sampleTable_collapsed <- data.frame(colData(myData)) # extract new sampleTable that has collapsed duplicates

# pre-filtering - filtering out empty rows. not at all necessary, but makes later computation faster.
myData <- myData[ rowSums(counts(myData)) > 1, ]

# relevel - this explicitly tells DESeq to consider samples labeled 'WT' the controls
myData$Genotype <- relevel(myData$Genotype, ref = 'WT')

# Make a data frame containing the raw Count data stored in the DESeqDataSet
rawCounts <- data.frame( counts(myData) )

```

# Batch Correction

## Calculate number of SVAs
```{r}
#import needed library
library('sva')

# make a full model matrix
mod  <- model.matrix(~ Sex + Genotype + GrowBatch, colData(myData))

# make a null model to compare it to
mod0 <- model.matrix(~   1, colData(myData))

# perform SVA without defining how many non-Genotype batch effects you think there are
n.sv <- svaseq( assay(myData), mod, mod0)  #as.matrix(rawCounts)

```


## Do any of these SVs line up with suspected batch effects?

Examine the extent to which SVs correlate with existing possible sources of batch.

CheckSVs function is a part of the DESeqAid package. 

```{r}
# Import needed library
library(DESeqAid)

# perform SVA when you specifically expect nSurr SVAs (number of SVAs must be less than number of samples)
n.sv <- 3
svseq <- svaseq( assay(myData), mod, mod0, n.sv)


## then deploy function
checkSVs(myData$SubjectID, svseq, n.sv)
checkSVs(myData$Genotype, svseq, n.sv) # shouldn't be a batch effect because this is accounted for in model
checkSVs(myData$Sex, svseq, n.sv)
checkSVs(myData$Line, svseq, n.sv)
checkSVs(myData$SeqBatch, svseq, n.sv)
checkSVs(myData$GrowBatch, svseq, n.sv) # maybe correlation
checkSVs(myData$VialID, svseq, n.sv)
checkSVs(myData$Lane, svseq, n.sv)
checkSVs(myData$Age, svseq, n.sv)
checkSVs(myData$IQ, svseq, n.sv)
checkSVs(myData$IQbin, svseq, n.sv)
checkSVs(myData$MI_pHH3_Pax6, svseq, n.sv)

```


## Update design - Choose number of SVs to use and remove from raw count data (for use in WCGNA, PCA)
```{r}

myData$SV1 <- svseq$sv[,1]
myData$SV2 <- svseq$sv[,2]
myData$SV3 <- svseq$sv[,3]

design(myData) <- ~ SV1 + SV2 + SV3 + GrowBatch+ Sex + Genotype

```


# Prepare data for visualization

## Make normalized versions of count data useful for clustering and visualization
```{r}
# VST normalization
myData_vst <- vst(myData)
myData_vst_df <- assay(myData_vst)

## make a filtered version
myData_vst_filt <- myData_vst[rowMeans(myData_vst_df) > 1,] # make a version that is filtered again


# save RData files
setwd(pathToRData)

save(myData_vst_df, file = paste0(currDate,'_',expName,'_vstCounts.RData'))
save(myData_vst_filt, file = paste0(currDate,'_',expName, '_vstCounts_Filtered.RData'))

# save text files
setwd(pathToProjectOutput)

write.table(myData_vst_df, file = paste0(currDate,'_',expName, '_rlogCounts.csv') )
write.table(assay(myData_vst_filt), file = paste0(currDate,'_',expName, '_rlogCounts_Filtered.csv') )

```

## Create batch corrected dataset
```{r}
# load needed libraries
library(limma)

# create new dds
myData_vst_bc <- myData_vst

## set up variables
limdesign <- model.matrix(~ Genotype + Sex + GrowBatch + SV1 + SV2 + SV3 , data=colData(myData_vst_bc))  # reminder: thing you care abt goes first
treatment.design <- limdesign[,1:2]
batch.design <- limdesign[,-(1:2)]

##'after' plots
assay(myData_vst_bc) <- limma::removeBatchEffect(assay(myData_vst_bc), design=treatment.design, covariates=batch.design) # remove batch

```



# Exploratory data analysis

## VST normalized, no batch correction

### PCA 
```{r}
# direct output to working directory
setwd(pathToProjectOutput)

# by genes
plotPCApcs_all <- plotPCA(myData_vst, 'Genotype', returnData=TRUE)
plotPCApcs <- plotPCApcs_all[,c(1:2)]
colnames(plotPCApcs) <- c('PC1', 'PC2')

# Main factors we're interested in
distPCA(plotPCApcs, sampleTable_collapsed$Genotype, 'Genotype', 'PC1', 'PC2', paste0(currDate, ' ', expName)) # strong effect
distPCA(plotPCApcs, sampleTable_collapsed$Sex, 'Sex', 'PC1', 'PC2', paste0(currDate, ' ', expName)) # strong effect
distPCA(plotPCApcs, sampleTable_collapsed$GrowBatch, 'GrowBatch', 'PC1', 'PC2', paste0(currDate, ' ', expName)) # strong effect

```

### How well are samples correlated?
```{r}
library('DESeqAid')

# plot sample correlation
sampSimilarityHeatmap(myData_vst_df, sampleTable_collapsed, c('Genotype', 'SubjectID', 'Sex', 'GrowBatch', 'Line'), sampleTable_collapsed$Line)

```


## VST normalized, batch correction (~Sex + Genotype + GrowBatch + SVs)

### PCA 
```{r}
# direct output to working directory
setwd(pathToProjectOutput)

# by genes
plotPCApcs_all <- plotPCA(myData_vst_bc, 'Genotype', returnData=TRUE)
plotPCApcs <- plotPCApcs_all[,c(1:2)]
colnames(plotPCApcs) <- c('PC1', 'PC2')

# Main factors we're interested in
distPCA(plotPCApcs, sampleTable_collapsed$Genotype, 'Genotype', 'PC1', 'PC2', paste0(currDate, ' ', expName)) 
distPCA(plotPCApcs, sampleTable_collapsed$Sex, 'Sex', 'PC1', 'PC2', paste0(currDate, ' ', expName))
distPCA(plotPCApcs, sampleTable_collapsed$GrowBatch, 'GrowBatch', 'PC1', 'PC2', paste0(currDate, ' ', expName)) 
```

### How well are samples correlated?
```{r}
# Load needed libraries
library('DESeqAid')

# plot sample correlation
sampSimilarityHeatmap(assay(myData_vst_bc), sampleTable_collapsed, c('Genotype', 'SubjectID', 'Sex', 'GrowBatch', 'Line'), sampleTable_collapsed$Line)

```


# 2019/06/28 : Study MIRS
```{r}

tst <- assay(myData_vst_bc)
tst2 <- assay(myData_vst)

tst_mir <- tst[grep("MIR", row.names(tst)),]
tst_mir2 <- tst2[grep("MIR", row.names(tst2)),]

tst_mir <- tst_mir[rev(order(rowSums(tst_mir))),] # order from most to least expressd
tst_non_mir <- tst[which(!(row.names(tst) %in% row.names(tst_mir))),]
hist(apply(tst, 1, mean))
hist(apply(tst_mir, 1, mean))
hist(apply(tst_non_mir, 1, mean))

# mean of non-mir means
nonMirMean <- mean(apply(tst_non_mir, 1, mean))
mirMeans <- apply(tst_mir, 1, mean)
which(mirMeans > nonMirMean)

# 
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("mirbase.db")

```


# HOUSEKEEPING
Run this block to save all your variables and store useful information about this run in the html file that accompanies this notebook.
```{r}
# print Session Info
sessionInfo()

# If you like it, save relevant files
setwd(pathToRData)
save(myData, file=paste0(currDate,'_', expName, '_myData.RData'))
save(myData_vst_bc, file=paste0(currDate,'_', expName, '_myData_vst_bc.RData'))

# Save all variables
save.image(file=paste0(currDate, '_', expName, '_allVars.RData'))

# save the things you'll need for DESeq
save(myData, myData_vst, myData_vst_bc, sampleTable_collapsed, userVars, file = paste0(currDate, '_', expName, '_DESeqWorkspace.RData'))

```




